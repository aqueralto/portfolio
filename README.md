<h1 align="center">Data Science and Programming Portfolio</h1>  
<p align="center">
  <img src="https://img.shields.io/badge/Python-3670A0?&logo=python&logoColor=ffffff" />
  <img src="https://img.shields.io/badge/NumPy-%23013243.svg?&logo=numpy&logoColor=white"/> 
  <img src="https://img.shields.io/badge/pandas-%23130754.svg?logo=pandas&logoColor=white"/>
  <img src="https://img.shields.io/badge/scikit--learn-%23F89939.svg?&logo=scikit-learn&logoColor=white"/>
  <img src="https://img.shields.io/badge/Plotly-%233F4F75.svg?&logo=plotly&logoColor=white"/>
  <img src="https://img.shields.io/badge/Jupyter-%23F37725.svg?&logo=jupyter&logoColor=white"/>  <br/> <br/>
  A collection of statistical and machine learning projects I've worked on for academic, self-learning, and professional purposes. 
</p>
<br/>

## Python [![python][1]][2]
### [Defining inkjet printing conditions of superconducting cuprate films through machine learning](https://github.com/aqueralto/ml_ijp_deposition_paper)
- Performed inkjet printing experiments and advanced characterizations to generate a dataset to perform machine learning.
- Analyzed feature distribution and correlations, preprocessed the data and engineered features to develop machine learning models.
- Developed 4 machine learning models, a linear regression that served as a benchmark model and 3 decision tree-based regression models (Random Forest, AdaBoost and Gradient Boosting).
- Predicted the number of drops deposited and the total volume required for optimal inkjet printing deposition with accuracies of 0.98-0.99.
- Identified the most important features,drop and line spacing, as well as drop volume, and their influence in the optimization process.

### [Predicting the Survival of Passangers from the Titanic]([https://www.kaggle.com/code/albertqueralto/surviving-titanic-with-ml])
- Performed feature engineering, preprocessing and exploratory data analysis on the Titanic dataset.
- Developed 5 machine learning models to compare their outputs. The best ones (Random Forest Classifier and XGBoost Classifier were selected for hyperparameter tuning. <b>Best score = 0.76315</b>.




<!-- icons -->
[1]: https://github.com/kellibelcher/kellibelcher/blob/main/images/python24.png (Python) 
[2]: https://www.python.org/
